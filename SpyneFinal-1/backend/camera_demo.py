# fast_demo.py
# Console-only posture/exercise/sport analysis with manual switching
# - j/k: switch actions within current category
# - 1/2/3: switch category
# - q: quit
# Feedback is always generated by Cerebras AI (LLM).
# Posture/exercise/sport choice is ALWAYS manual (no auto-detection).

import time
import cv2
import mediapipe as mp
from pose_logic import analyze
from api import analyze_with_llm

# -------------------------
# Settings
# -------------------------
FRAME_WIDTH = 640
FRAME_HEIGHT = 480

# Categories and actions
CATEGORIES = {
    "POSTURE": ["sitting", "standing", "Computer Work"],
    "EXERCISE": ["squat", "pushup", "bicep_curl"],
    "SPORT": ["jumping_jack", "throw"],
}

DISPLAY_NAMES = {
    "POSTURE": "Posture",
    "EXERCISE": "Exercise",
    "SPORT": "Sport",
}

# -------------------------
# MediaPipe setup
# -------------------------
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

def landmarks_to_dict(landmarks):
    """Convert MediaPipe landmarks to {NAME: (x_norm, y_norm)} dict."""
    kps = {}
    for lm in mp_pose.PoseLandmark:
        pt = landmarks[lm.value]
        kps[lm.name] = (float(pt.x), float(pt.y))
    return kps

# -------------------------
# Main
# -------------------------
def main():
    print("=== AR PT Coach (Console Mode) ===")
    print("Keys: 1=POSTURE | 2=EXERCISE | 3=SPORT | j=prev | k=next | q=quit")

    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)

    if not cap.isOpened():
        print("Cannot open camera.")
        return

    category = "POSTURE"
    action_idx = 0

    with mp_pose.Pose(
        static_image_mode=False,
        model_complexity=1,
        enable_segmentation=False,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
    ) as pose:

        while True:
            ok, frame = cap.read()
            if not ok:
                break

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            res = pose.process(rgb)

            chosen_action = CATEGORIES[category][action_idx % len(CATEGORIES[category])]

            if res.pose_landmarks:
                kps = landmarks_to_dict(res.pose_landmarks.landmark)

                # Call Cerebras for feedback (but NEVER override chosen_action)
                status, feedback, score, metrics, _ = analyze_with_llm(category, chosen_action, kps)
                act = chosen_action  # manual choice always wins

                # Console output
                print("==================================================")
                print(f"[{time.strftime('%H:%M:%S')}] Mode={DISPLAY_NAMES[category]} / Action={act}")
                print(f"Score: {score} ({status})")
                for tip in feedback[:3]:
                    print(f" - {tip}")
            else:
                print("==================================================")
                print("No person detected.")

            # Key controls
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('1'):
                category, action_idx = "POSTURE", 0
            elif key == ord('2'):
                category, action_idx = "EXERCISE", 0
            elif key == ord('3'):
                category, action_idx = "SPORT", 0
            elif key == ord('j'):
                action_idx = (action_idx - 1) % len(CATEGORIES[category])
            elif key == ord('k'):
                action_idx = (action_idx + 1) % len(CATEGORIES[category])

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
